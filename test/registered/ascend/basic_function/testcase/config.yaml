model-path: /root/.cache/modelscope/hub/models/Qwen/Qwen3-32B
attention-backend: ascend
trust-remote-code: true
disable-cuda-graph: true
mem-fraction-static: 0.9
tp-size: 4
forward-hooks: |
  [
    {
      "Name": "qwen_first_layer_attn_monitor",
      "target_modules":,
      "hook_factory": "monitor:create_attention_monitor_factory",
      "config": {
        "layer_index": 0
      }
    }
  ]


